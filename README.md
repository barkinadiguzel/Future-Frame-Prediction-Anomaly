# ğŸ¥ Future Frame Prediction for Video Anomaly Detection

This repository provides a **theoretical PyTorch implementation** of the  
**Future Frame Prediction framework for video anomaly detection**.

The goal is not leaderboard performance, but to **translate the original paperâ€™s ideas into clean, understandable code**, focusing on *why* anomaly detection emerges from prediction failure rather than classification.

- Learning **normal motion and appearance dynamics** from video sequences ğŸ«§  
- Predicting the **next video frame** from past observations ğŸŒ’  
- Detecting anomalies via **prediction inconsistency** instead of labels ğŸ§¿  
- Loss-driven temporal understanding without explicit temporal models ğŸ§©  

**Paper reference:** [Future Frame Prediction for Anomaly Detection â€“ Wen Liu et al., CVPR 2018](https://arxiv.org/pdf/1712.09867) ğŸª


---

## ğŸŒŒ Overview â€“ Predict the Future, Expose the Abnormal

![Figure Mix](images/figmix.jpg)

The central idea is deceptively simple:

> A model that has only seen *normal futures* will fail when the future becomes abnormal.

Instead of classifying events, the system **predicts what should happen next** and measures how wrong that prediction is.

High error â†’ something unusual is happening.

### Core pipeline (condensed from the paper):

1. Given a sequence of past frames:
   $$X_t = \{x_{t-k}, \dots, x_{t-1}\}$$

2. Predict the next frame using a convolutional encoderâ€“decoder:
   $$\hat{x}_t = G(X_t)$$

3. Compare predicted frame $\hat{x}_t$ with the true future frame $x_t$

4. Compute multiple complementary prediction losses:
   - **Intensity loss** (pixel accuracy)
   - **Gradient loss** (edge & structure consistency)
   - **Optical flow loss** (motion consistency)

5. Combine losses into a single anomaly score:
   $$A_t = \alpha L_{\text{int}} + \beta L_{\text{grad}} + \gamma L_{\text{flow}}$$

During inference:
- Low $A_t$ â†’ normal behavior  
- High $A_t$ â†’ anomaly candidate  

No labels. No classifiers.ğŸ”

---

## ğŸ§  What the Model Actually Learns

The predictor network learns:

- **Appearance regularities** (what normal frames look like)
- **Motion regularities** (how objects usually move)
- **Temporal smoothness** (what changes are expected vs. suspicious)

Importantly:
- The model is **never told what an anomaly is**
- Anomalies emerge naturally when prediction **breaks down**

Think of it as learning the *grammar* of normal motion â€” anomalies are grammatical errors ğŸª¶

---

## ğŸ§® Loss Functions (Paper-Aligned, Short Form)

### 1. Intensity Loss (Eq. 1)
Pixel-wise difference between prediction and ground truth:

$$L_{\text{int}} = \| \hat{x}_t - x_t \|_2$$

Encourages accurate reconstruction of normal frames.

---

### 2. Gradient Loss (Eq. 2)
Difference between spatial gradients:

$$L_{\text{grad}} = \| \nabla \hat{x}_t - \nabla x_t \|_1$$

Preserves edges and structural sharpness, preventing blurry predictions.

---

### 3. Optical Flow Loss (Eq. 3)
Difference between motion fields:

$$L_{flow} = \| F(x^t, x_{t-1}) - F(x_t, x_{t-1}) \|_1$$


Ensures predicted motion matches real motion patterns.

This is where **temporal understanding** truly enters the system ğŸŒŠ

---

## ğŸ§© Why No Conv3D or ConvLSTM?

Deliberately avoided.

- Temporal reasoning is enforced **through losses**, not architecture  
- Keeps the model lightweight and interpretable  
- Strengthens the paperâ€™s claim: *prediction failure alone is enough*

This makes the framework a **baseline philosophy**, not a brute-force solution âš–ï¸

---

## ğŸ“¦ Repository Structure

```bash
Future-Frame-Prediction-Anomaly/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_block.py          # Conv2D + activation
â”‚   â”‚   â”œâ”€â”€ deconv_block.py        # ConvTranspose2D
â”‚   â”‚   â””â”€â”€ utils_layers.py        # weight init, padding helpers
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â”œâ”€â”€ encoder_block.py       # U-Net encoder (downsampling)
â”‚   â”‚   â””â”€â”€ decoder_block.py       # U-Net decoder (upsampling + skip)
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ future_frame_model.py  # U-Net based future frame predictor
â”‚   â”‚   â””â”€â”€ generator.py           # optional wrapper, if needed
â”‚   â”‚
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”œâ”€â”€ intensity_loss.py      # L2 pixel loss
â”‚   â”‚   â”œâ”€â”€ gradient_loss.py       # Gradient loss
â”‚   â”‚   â””â”€â”€ optical_flow_loss.py   # Flow consistency loss
â”‚   â”‚
â”‚   â””â”€â”€ config.py                  # Î»_int, Î»_gd, Î»_op
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
